This task contains two square coordinate excerpts from subtropical
climate zones:
1. In the US Southeast --> around Louisana, Arkansas, etc.
2. In Southeastern China

Goals:
1. The first goal is to train the model on the surface data and let it predict data at "flight level"

2.Another goal is to train the models on one (maybe the US) and let it predict
on the Chinese data. By this, we can see how the model is able to infer general
climate data mechansim and not relying on the place itself.

The data is obtained from https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=form
and the variables are:
2m temperature
Surface pressure
10m u-component of wind
10m v-component of wind
100m u-component of wind
100m v-component of wind
Oreography (geopotential)

Years: 1980 - 2018
Months: December, January, February
Days, Time: All

--> 3*24*30*38=82 000 data samples

The North-South difference in degrees is: 10
The West-East difference in degrees is: 10

The resolution of the data set is 0.25 degree --> we get 40 grid points per axis, i.e. one sample has 
160 points --> which is perfect for comparison.

Positive longitudes are east of the prime meridian, and negative ones are west.

For the US: We choose the following coordinates:
West: 96 W -96
North:40 N +40
East: 86 W -86
South: 30 N +30

i.e. Alabama, Louisana, Arkansas, Tennessee, Mississippi


For China: We choose the following coordinates:
West: 105 E +105
North: 35 N +35
East: 115 E +115
South: 25 N +25

i.e. the Southeastern "Non-Coast" region of China

#For China: An alternative region might be:
#West: 105 E +105
#North: 35 N +35
#East: 115 E +115
#South: 25 N +25
#This would be further east but the topological properties of the region but be more similiar to what we see in the US.


We did some test to count the number of rows in the processes CSV files:
Difference between unformatted and unprocessed:
#The number of deleted rows is for 1980: 25714416-25699129=15287 (1980 was a leap year) 15288/168=91
#The number of deleted rows is for 1981: 25431840-25416721=15119 (difference to 1980: 168) 15119/168=81
#The number of deleted rows is for 1982: 25431840-25416721=15119 (difference to 1980: 168) 15119/168=81
#The number of deleted rows is for 1983: 25431840-25416721=15119 (difference to 1980: 168) 15119/168=81

It seems to be consist! --> Format_CSV_ERA5.py seems to be correct (at least consistent throughout files.)


There are two types of files:
1. Small - test,valid,train - train consists of years 2014,2015,2016
			      valid consists of years 2017
			      test consists of years 2018
2. Big - test,valid,train - train consists of years ...

